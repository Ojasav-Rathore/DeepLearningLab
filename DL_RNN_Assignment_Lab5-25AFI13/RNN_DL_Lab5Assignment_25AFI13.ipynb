{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14826823,"sourceType":"datasetVersion","datasetId":9482565}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom collections import Counter\nimport re\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T09:46:20.733526Z","iopub.execute_input":"2026-02-13T09:46:20.734217Z","iopub.status.idle":"2026-02-13T09:46:20.739483Z","shell.execute_reply.started":"2026-02-13T09:46:20.734188Z","shell.execute_reply":"2026-02-13T09:46:20.738623Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class RNNNumPy:\n    def __init__(self, vocab_size, hidden_size):\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        \n        self.W_hx = np.random.randn(hidden_size, vocab_size) * 0.01\n\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n\n        self.W_hy = np.random.randn(vocab_size, hidden_size) * 0.01\n        \n        self.b_h = np.zeros((hidden_size, 1)) \n        self.b_y = np.zeros((vocab_size, 1))  \n        \n    def forward(self, inputs):\n        h_states = {}\n        outputs = {}\n        h_states[-1] = np.zeros((self.hidden_size, 1))\n        \n        loss = 0\n        for t, word_idx in enumerate(inputs):\n            x_t = np.zeros((self.vocab_size, 1))\n            x_t[word_idx] = 1\n            \n            h_states[t] = np.tanh(np.dot(self.W_hx, x_t) + np.dot(self.W_hh, h_states[t-1]) + self.b_h)\n            \n            y_t = np.dot(self.W_hy, h_states[t]) + self.b_y\n\n            outputs[t] = np.exp(y_t) / np.sum(np.exp(y_t))\n            \n        return outputs, h_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T09:46:20.741011Z","iopub.execute_input":"2026-02-13T09:46:20.741326Z","iopub.status.idle":"2026-02-13T09:46:20.754910Z","shell.execute_reply.started":"2026-02-13T09:46:20.741304Z","shell.execute_reply":"2026-02-13T09:46:20.754251Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/poems-100-csv/poems-100 - poems-100.csv')\ntext_data = \" \".join(df['text'].dropna().tolist())\n\n\ndef tokenize(text):\n    text = text.lower()\n    words = re.findall(r'\\b\\w+\\b|[.,!?;]', text)\n    return words\n\ntokens = tokenize(text_data)\n\nword_counts = Counter(tokens)\nvocab = sorted(word_counts, key=word_counts.get, reverse=True)\nvocab_size = len(vocab)\n\nword2idx = {word: idx for idx, word in enumerate(vocab)}\nidx2word = {idx: word for idx, word in enumerate(vocab)}\n\nprint(f\"Total tokens: {len(tokens)}\")\nprint(f\"Vocabulary size: {vocab_size}\")\n\nseq_length = 20\ndata_x = []\ndata_y = []\n\nfor i in range(0, len(tokens) - seq_length):\n    seq_in = tokens[i : i + seq_length]\n    seq_out = tokens[i + 1 : i + seq_length + 1] # Predict the next word in the sequence\n    data_x.append([word2idx[word] for word in seq_in])\n    data_y.append([word2idx[word] for word in seq_out])\n\nX_tensor = torch.tensor(data_x, dtype=torch.long)\nY_tensor = torch.tensor(data_y, dtype=torch.long)\n\nfrom torch.utils.data import TensorDataset, DataLoader\ndataset = TensorDataset(X_tensor, Y_tensor)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T09:46:20.755715Z","iopub.execute_input":"2026-02-13T09:46:20.755954Z","iopub.status.idle":"2026-02-13T09:46:21.013708Z","shell.execute_reply.started":"2026-02-13T09:46:20.755934Z","shell.execute_reply":"2026-02-13T09:46:21.013113Z"}},"outputs":[{"name":"stdout","text":"Total tokens: 29175\nVocabulary size: 5163\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"class RNNOneHot(nn.Module):\n    def __init__(self, vocab_size, hidden_size, num_layers=1):\n        super(RNNOneHot, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.rnn = nn.RNN(input_size=vocab_size, hidden_size=hidden_size, \n                          num_layers=num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x, hidden):\n        x_one_hot = F.one_hot(x, num_classes=self.vocab_size).float()\n        \n        out, hidden = self.rnn(x_one_hot, hidden)\n        out = self.fc(out)\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n\nhidden_dim = 128\nmodel_onehot = RNNOneHot(vocab_size, hidden_dim).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer_onehot = optim.Adam(model_onehot.parameters(), lr=0.005)\n\nepochs = 50\nprint(\"Training--One-Hot--RNN\")\nfor epoch in range(epochs):\n    model_onehot.train()\n    total_loss = 0\n    \n    for batch_x, batch_y in dataloader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        \n        hidden = model_onehot.init_hidden(batch_x.size(0))\n        \n        optimizer_onehot.zero_grad()\n        output, hidden = model_onehot(batch_x, hidden)\n        \n        loss = criterion(output.view(-1, vocab_size), batch_y.view(-1))\n        loss.backward()\n        optimizer_onehot.step()\n        \n        total_loss += loss.item()\n        \n    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T09:46:21.015422Z","iopub.execute_input":"2026-02-13T09:46:21.015738Z","iopub.status.idle":"2026-02-13T09:47:56.091834Z","shell.execute_reply.started":"2026-02-13T09:46:21.015717Z","shell.execute_reply":"2026-02-13T09:47:56.091065Z"}},"outputs":[{"name":"stdout","text":"Training--One-Hot--RNN\nEpoch [1/50], Loss: 5.5285\nEpoch [2/50], Loss: 1.8528\nEpoch [3/50], Loss: 0.6141\nEpoch [4/50], Loss: 0.4359\nEpoch [5/50], Loss: 0.3842\nEpoch [6/50], Loss: 0.3610\nEpoch [7/50], Loss: 0.3471\nEpoch [8/50], Loss: 0.3404\nEpoch [9/50], Loss: 0.3346\nEpoch [10/50], Loss: 0.3294\nEpoch [11/50], Loss: 0.3259\nEpoch [12/50], Loss: 0.3232\nEpoch [13/50], Loss: 0.3195\nEpoch [14/50], Loss: 0.3185\nEpoch [15/50], Loss: 0.3169\nEpoch [16/50], Loss: 0.3138\nEpoch [17/50], Loss: 0.3119\nEpoch [18/50], Loss: 0.3110\nEpoch [19/50], Loss: 0.3097\nEpoch [20/50], Loss: 0.3074\nEpoch [21/50], Loss: 0.3069\nEpoch [22/50], Loss: 0.3064\nEpoch [23/50], Loss: 0.3052\nEpoch [24/50], Loss: 0.3041\nEpoch [25/50], Loss: 0.3035\nEpoch [26/50], Loss: 0.3026\nEpoch [27/50], Loss: 0.3017\nEpoch [28/50], Loss: 0.3006\nEpoch [29/50], Loss: 0.3002\nEpoch [30/50], Loss: 0.2998\nEpoch [31/50], Loss: 0.2987\nEpoch [32/50], Loss: 0.2989\nEpoch [33/50], Loss: 0.2982\nEpoch [34/50], Loss: 0.2979\nEpoch [35/50], Loss: 0.2977\nEpoch [36/50], Loss: 0.2965\nEpoch [37/50], Loss: 0.2954\nEpoch [38/50], Loss: 0.2962\nEpoch [39/50], Loss: 0.2960\nEpoch [40/50], Loss: 0.2950\nEpoch [41/50], Loss: 0.2951\nEpoch [42/50], Loss: 0.2944\nEpoch [43/50], Loss: 0.2942\nEpoch [44/50], Loss: 0.2936\nEpoch [45/50], Loss: 0.2937\nEpoch [46/50], Loss: 0.2929\nEpoch [47/50], Loss: 0.2923\nEpoch [48/50], Loss: 0.2925\nEpoch [49/50], Loss: 0.2924\nEpoch [50/50], Loss: 0.2920\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"class RNNEmbedding(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n        super(RNNEmbedding, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.RNN(input_size=embed_size, hidden_size=hidden_size, \n                          num_layers=num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x, hidden):\n        embedded = self.embedding(x)\n        out, hidden = self.rnn(embedded, hidden)\n        out = self.fc(out)\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n\nembed_dim = 64\nmodel_embed = RNNEmbedding(vocab_size, embed_dim, hidden_dim).to(device)\noptimizer_embed = optim.Adam(model_embed.parameters(), lr=0.005)\n\nprint(\"\\n Training--Embedding--RNN\")\nfor epoch in range(epochs):\n    model_embed.train()\n    total_loss = 0\n    \n    for batch_x, batch_y in dataloader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        \n        hidden = model_embed.init_hidden(batch_x.size(0))\n        \n        optimizer_embed.zero_grad()\n        output, hidden = model_embed(batch_x, hidden)\n        \n        loss = criterion(output.view(-1, vocab_size), batch_y.view(-1))\n        loss.backward()\n        optimizer_embed.step()\n        \n        total_loss += loss.item()\n        \n    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T09:47:56.092972Z","iopub.execute_input":"2026-02-13T09:47:56.093370Z","iopub.status.idle":"2026-02-13T09:49:04.593612Z","shell.execute_reply.started":"2026-02-13T09:47:56.093343Z","shell.execute_reply":"2026-02-13T09:49:04.592865Z"}},"outputs":[{"name":"stdout","text":"\n Training--Embedding--RNN\nEpoch [1/50], Loss: 4.1066\nEpoch [2/50], Loss: 1.4739\nEpoch [3/50], Loss: 0.8581\nEpoch [4/50], Loss: 0.6478\nEpoch [5/50], Loss: 0.5510\nEpoch [6/50], Loss: 0.5016\nEpoch [7/50], Loss: 0.4713\nEpoch [8/50], Loss: 0.4517\nEpoch [9/50], Loss: 0.4390\nEpoch [10/50], Loss: 0.4271\nEpoch [11/50], Loss: 0.4209\nEpoch [12/50], Loss: 0.4147\nEpoch [13/50], Loss: 0.4094\nEpoch [14/50], Loss: 0.4037\nEpoch [15/50], Loss: 0.4003\nEpoch [16/50], Loss: 0.3984\nEpoch [17/50], Loss: 0.3941\nEpoch [18/50], Loss: 0.3912\nEpoch [19/50], Loss: 0.3897\nEpoch [20/50], Loss: 0.3864\nEpoch [21/50], Loss: 0.3860\nEpoch [22/50], Loss: 0.3823\nEpoch [23/50], Loss: 0.3824\nEpoch [24/50], Loss: 0.3802\nEpoch [25/50], Loss: 0.3785\nEpoch [26/50], Loss: 0.3771\nEpoch [27/50], Loss: 0.3768\nEpoch [28/50], Loss: 0.3741\nEpoch [29/50], Loss: 0.3748\nEpoch [30/50], Loss: 0.3738\nEpoch [31/50], Loss: 0.3718\nEpoch [32/50], Loss: 0.3717\nEpoch [33/50], Loss: 0.3698\nEpoch [34/50], Loss: 0.3706\nEpoch [35/50], Loss: 0.3691\nEpoch [36/50], Loss: 0.3683\nEpoch [37/50], Loss: 0.3693\nEpoch [38/50], Loss: 0.3656\nEpoch [39/50], Loss: 0.3667\nEpoch [40/50], Loss: 0.3664\nEpoch [41/50], Loss: 0.3656\nEpoch [42/50], Loss: 0.3642\nEpoch [43/50], Loss: 0.3641\nEpoch [44/50], Loss: 0.3625\nEpoch [45/50], Loss: 0.3638\nEpoch [46/50], Loss: 0.3638\nEpoch [47/50], Loss: 0.3636\nEpoch [48/50], Loss: 0.3613\nEpoch [49/50], Loss: 0.3629\nEpoch [50/50], Loss: 0.3628\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def generate_text(model, start_text, num_words, is_one_hot=False):\n    model.eval()\n    words = tokenize(start_text)\n    \n    hidden = model.init_hidden(1)\n    \n    for _ in range(num_words):\n        x_idx = [word2idx.get(w, word2idx[vocab[0]]) for w in words[-seq_length:]]\n        x_tensor = torch.tensor([x_idx], dtype=torch.long).to(device)\n        \n        with torch.no_grad():\n            output, hidden = model(x_tensor, hidden)\n            \n            last_word_logits = output[0, -1, :]\n\n            probs = F.softmax(last_word_logits, dim=0).cpu().numpy()\n            predicted_idx = np.random.choice(len(probs), p=probs)\n            \n            words.append(idx2word[predicted_idx])\n            \n    return \" \".join(words)\n\nseed_text = \"the rose is red\"\nprint(\"\\nOne-Hot Generation:\")\nprint(generate_text(model_onehot, seed_text, num_words=30, is_one_hot=True))\n\nprint(\"\\nEmbedding Generation:\")\nprint(generate_text(model_embed, seed_text, num_words=30, is_one_hot=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T09:49:04.594526Z","iopub.execute_input":"2026-02-13T09:49:04.594748Z","iopub.status.idle":"2026-02-13T09:49:04.643738Z","shell.execute_reply.started":"2026-02-13T09:49:04.594726Z","shell.execute_reply":"2026-02-13T09:49:04.642897Z"}},"outputs":[{"name":"stdout","text":"\nOne-Hot Generation:\nthe rose is red , the violet s blue , sugar is sweet , and so are you . how do i love thee ? let me count the ways . i love thee\n\nEmbedding Generation:\nthe rose is red , the violet s blue , sugar is sweet , and so in man , o lord , to him , in vain the ocean settling in hollows and the\n","output_type":"stream"}],"execution_count":17}]}