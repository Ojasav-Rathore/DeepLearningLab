{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6XeLLYLxuIuir+R81JPI8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "qkW851P2-_uq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_string(s):\n",
        "    s = s.lower().strip() # Lowercasing\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "        self.idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
        "        self.idx = 4\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "\n",
        "# Load and sample dataset\n",
        "def load_dataset(filepath, num_samples=10000):\n",
        "    eng_sentences, spa_sentences = [], []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Sample subset\n",
        "    lines = lines[:num_samples]\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.split('\\t') # Separated by a tab\n",
        "        if len(parts) >= 2:\n",
        "            eng = normalize_string(parts[0])\n",
        "            spa = normalize_string(parts[1])\n",
        "            eng_sentences.append(eng)\n",
        "            spa_sentences.append(spa)\n",
        "\n",
        "    return eng_sentences, spa_sentences\n",
        "\n",
        "eng_data, spa_data = load_dataset('spa.txt', num_samples=10000)\n",
        "\n",
        "eng_vocab = Vocab()\n",
        "spa_vocab = Vocab()\n",
        "\n",
        "for eng, spa in zip(eng_data, spa_data):\n",
        "    eng_vocab.add_sentence(eng)\n",
        "    spa_vocab.add_sentence(spa)"
      ],
      "metadata": {
        "id": "By-22c7gBnew"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_data, trg_data, src_vocab, trg_vocab, max_len=50):\n",
        "        self.src_data = src_data\n",
        "        self.trg_data = trg_data\n",
        "        self.src_vocab = src_vocab\n",
        "        self.trg_vocab = trg_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def process_sentence(self, sentence, vocab):\n",
        "        tokens = [vocab.word2idx.get(word, vocab.word2idx[\"<unk>\"]) for word in sentence.split(' ')]\n",
        "        tokens = [vocab.word2idx[\"<sos>\"]] + tokens + [vocab.word2idx[\"<eos>\"]]\n",
        "        # Padding\n",
        "        if len(tokens) < self.max_len:\n",
        "            tokens += [vocab.word2idx[\"<pad>\"]] * (self.max_len - len(tokens))\n",
        "        return torch.tensor(tokens[:self.max_len])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.process_sentence(self.src_data[idx], self.src_vocab)\n",
        "        trg = self.process_sentence(self.trg_data[idx], self.trg_vocab)\n",
        "        return src, trg\n",
        "\n",
        "dataset = TranslationDataset(eng_data, spa_data, eng_vocab, spa_vocab)\n",
        "\n",
        "# Splitting 80% Train, 10% Val, 10% Test\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "-ZBLAc4jB5hP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Sinusoidal positional encoding\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Adding positional encoding to input embeddings\n",
        "        return x + self.pe[:, :x.size(1), :]"
      ],
      "metadata": {
        "id": "YWmTfvA9_Fan"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Masking for padded tokens or future tokens in decoder\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention, V)\n",
        "        return output\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        Q = self.W_q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        # Extend to multi-head attention\n",
        "        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        return self.W_o(attention_output)"
      ],
      "metadata": {
        "id": "gOS45ech_Kk8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Multi-head self-attention layer with residual connection and layer normalization\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Position-wise Feed Forward Network with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ih4MR3N__Qb2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, trg_mask):\n",
        "        # Masked multi-head self-attention\n",
        "        self_attn_output = self.self_attn(x, x, x, trg_mask)\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "\n",
        "        # Encoder-Decoder (cross) attention\n",
        "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "\n",
        "        # Feed Forward Network\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "JReMg_D8_UXn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5VOrZWS2PnId"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model=512, num_heads=8, num_layers=6, d_ff=2048, max_len=5000, dropout=0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "        # 1. Embedding Layer\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Stack multiple encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        # Stack multiple decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def make_src_mask(self, src, src_pad_idx):\n",
        "        src_mask = (src != src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_trg_mask(self, trg, trg_pad_idx):\n",
        "        trg_pad_mask = (trg != trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool()\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg, src_pad_idx, trg_pad_idx):\n",
        "        src_mask = self.make_src_mask(src, src_pad_idx)\n",
        "        trg_mask = self.make_trg_mask(trg, trg_pad_idx)\n",
        "\n",
        "        # Source embedding and positional encoding\n",
        "        src_emb = self.dropout(self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model)))\n",
        "\n",
        "        # Pass through Encoder\n",
        "        enc_output = src_emb\n",
        "        for layer in self.encoder_layers:\n",
        "            enc_output = layer(enc_output, src_mask)\n",
        "\n",
        "        # Target embedding and positional encoding\n",
        "        trg_emb = self.dropout(self.pos_encoder(self.trg_embedding(trg) * math.sqrt(self.d_model)))\n",
        "\n",
        "        # Pass through Decoder\n",
        "        dec_output = trg_emb\n",
        "        for layer in self.decoder_layers:\n",
        "            dec_output = layer(dec_output, enc_output, src_mask, trg_mask)\n",
        "\n",
        "        output = self.fc_out(dec_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "PAD_IDX = eng_vocab.word2idx[\"<pad>\"]\n",
        "\n",
        "model = Seq2SeqTransformer(\n",
        "    src_vocab_size=eng_vocab.idx,\n",
        "    trg_vocab_size=spa_vocab.idx,\n",
        "    d_model=256,\n",
        "    num_heads=8,\n",
        "    num_layers=3,\n",
        "    d_ff=512,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "# Optimizer & Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005) # Adam optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) # Cross-Entropy with padding mask"
      ],
      "metadata": {
        "id": "_w0oGKm-BGCE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Teacher forcing: Decoder input is everything except last token\n",
        "        trg_input = trg[:, :-1]\n",
        "        # Target for loss is everything except first token (<sos>)\n",
        "        trg_target = trg[:, 1:]\n",
        "\n",
        "        output = model(src, trg_input, PAD_IDX, PAD_IDX)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg_target = trg_target.contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, trg_target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# Training execution\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ODXyRpPjCb1n",
        "outputId": "2ecf2434-d0a5-4d2d-ff4a-9734f0901748"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (50) must match the size of tensor b (1600) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-510/1387969390.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-510/1387969390.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrg_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD_IDX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD_IDX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-510/3687813097.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, src_pad_idx, trg_pad_idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Target embedding and positional encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-510/3677406327.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Multi-head self-attention layer with residual connection and layer normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Position-wise Feed Forward Network with residual connection and layer normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (50) must match the size of tensor b (1600) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(test_loader, model, eng_vocab, spa_vocab, device):\n",
        "    model.eval()\n",
        "    targets = []\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in test_loader:\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            # Greedy decoding for evaluation\n",
        "            batch_size = src.shape[0]\n",
        "            trg_indexes = torch.full((batch_size, 1), spa_vocab.word2idx[\"<sos>\"], dtype=torch.long, device=device)\n",
        "\n",
        "            for _ in range(50): # Max len\n",
        "                output = model(src, trg_indexes, PAD_IDX, PAD_IDX)\n",
        "                pred_token = output.argmax(2)[:, -1].unsqueeze(1)\n",
        "                trg_indexes = torch.cat((trg_indexes, pred_token), dim=1)\n",
        "\n",
        "            # Convert indices to words for BLEU calculation\n",
        "            for i in range(batch_size):\n",
        "                pred_words = []\n",
        "                for idx in trg_indexes[i, 1:]:\n",
        "                    if idx.item() == spa_vocab.word2idx[\"<eos>\"]: break\n",
        "                    pred_words.append(spa_vocab.idx2word[idx.item()])\n",
        "                predictions.append(pred_words)\n",
        "\n",
        "                trg_words = []\n",
        "                for idx in trg[i, 1:]:\n",
        "                    if idx.item() == spa_vocab.word2idx[\"<eos>\"]: break\n",
        "                    if idx.item() != PAD_IDX:\n",
        "                        trg_words.append(spa_vocab.idx2word[idx.item()])\n",
        "                targets.append([trg_words])\n",
        "\n",
        "    bleu_score = corpus_bleu(targets, predictions) * 100\n",
        "    return bleu_score\n",
        "\n",
        "# Evaluate translation performance using BLEU score\n",
        "bleu = calculate_bleu(test_loader, model, eng_vocab, spa_vocab, device)\n",
        "print(f'Test BLEU Score: {bleu:.2f}')"
      ],
      "metadata": {
        "id": "ERY5sZ1ZCgf2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}